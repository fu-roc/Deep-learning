{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ],
   "id": "b91bfc3c9e13f69e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, key_size=256, query_size=256, value_size=256, hid_in_features=256, mlm_in_features=256, nsp_in_features=256)\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
    "    return bert, vocab"
   ],
   "id": "d2e50821db9afe9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "devices =d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(pretrained_model='bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4, num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ],
   "id": "d5fb1d275119b76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b6e0fc4d24b15c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, h_tokens in zip(*[d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])]\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "    \n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        # pool = multiprocessing.Pool(16)\n",
    "        # out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        out = map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        out = list(out)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return torch.tensor(all_token_ids, dtype=torch.long), torch.tensor(all_segments, dtype=torch.long), valid_lens\n",
    "    \n",
    "    def _mp_worker(self, all_premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = all_premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "    \n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "id": "f94e8f0ebdb35d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "# data_dir = d2l.download_extract('SNLI')\n",
    "data_dir = '../data/snli_1.0'\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ],
   "id": "ab244ff5c7f4c542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        token_X, segments_X, valid_lens = inputs\n",
    "        encoded_X = self.encoder(token_X, segments_X, valid_lens)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ],
   "id": "1589358bb75fc1fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "net = BERTClassifier(bert)",
   "id": "ebf51d956ed3ea47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ],
   "id": "8695b6ee23aea872",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:38:56.049477Z",
     "start_time": "2024-08-07T00:38:52.240560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.nn.functional.linear.__doc__)\n"
   ],
   "id": "c435ce774e0e2518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "linear(input, weight, bias=None) -> Tensor\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>`\n",
      "\n",
      "\n",
      ".. warning::\n",
      "    Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,\n",
      "    or may not have autograd support. If you notice missing functionality please\n",
      "    open a feature request.\n",
      "\n",
      "This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "Shape:\n",
      "\n",
      "    - Input: :math:`(*, in\\_features)` where `*` means any number of\n",
      "      additional dimensions, including none\n",
      "    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n",
      "    - Bias: :math:`(out\\_features)` or :math:`()`\n",
      "    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:41:23.933604Z",
     "start_time": "2024-08-07T00:41:23.924602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.linear)\n"
   ],
   "id": "c8c09d63de2e1c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function linear>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:41:37.953197Z",
     "start_time": "2024-08-07T00:41:37.940193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(torch._C._nn.linear)\n"
   ],
   "id": "5ad634ccc9c76c69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function linear>\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
